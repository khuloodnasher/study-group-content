{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics Review \n",
    "\n",
    "![](https://raw.githubusercontent.com/learn-co-curriculum/dsc-logistic-regression-topic-questions/master/visuals/cnf_matrix.png?token=AK7GP7ETVQ2IEUER2E5I7ZC6YWIVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using the confusion matrix up above, calculate precision, recall, and F-1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T18:19:36.872841Z",
     "start_time": "2020-05-14T18:19:36.861653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prcision is: 0.88\n",
      "recal is: 0.71\n",
      "f1 is: 0.79\n",
      "accuracy is: 0.84\n"
     ]
    }
   ],
   "source": [
    "#Use python to answer question here \n",
    "\n",
    "TP = 30\n",
    "TN = 54\n",
    "FP = 4\n",
    "FN = 12\n",
    "\n",
    "accuracy= (TP+TN)/(TP+FP+FN+TN)\n",
    "precision= TP/(TP+FP)\n",
    "recall= TP/(TP+FN)\n",
    "f_1_score=2*precision*recall/(precision+recall)\n",
    "print('prcision is:',round(precision,2))\n",
    "print('recal is:',round(recall,2))\n",
    "print('f1 is:',round(f_1_score,2))\n",
    "print('accuracy is:',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is a real life example of when you would care more about recall than precision? Make sure to include information about errors in your explanation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:23:50.584656Z",
     "start_time": "2020-05-13T20:23:50.582274Z"
    }
   },
   "source": [
    "### Your written answer here \n",
    "To answer this question, \n",
    "We need to know where false negative is more crucial than false positive.\n",
    "The difference between precision and recall is only in the denominator. In recall's denominator I have FN, while in Precision denominator I have FP. In health care, I’m concerned to get the right diagnosis. So, if my model predicts a patient has negative results in his sickness while the truth is he is sick, this prediction might end the life of the patient. Since recall metric includes False negative, Recall is more important in health care. On other hand, in business I’m trying to find if this business is profitable or not, I want a metric that has FP which is in precision. If my model predicts this business is profitable but it actually is not profitable which is False positive. My model will cause a loss to people who will use it. In this case, precision in business is more important than recall and I want to have low false positive result in my model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pick the best ROC curve from this graph and explain your choice.\n",
    "Note: each ROC curve represents one model, each labeled with the feature(s) inside each model.\n",
    "![](https://raw.githubusercontent.com/learn-co-curriculum/dsc-logistic-regression-topic-questions/master/visuals/many_roc.png?token=AK7GP7AHF62PSLZVQDC5DWK6YWJCQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:25:51.688614Z",
     "start_time": "2020-05-13T20:25:51.686226Z"
    }
   },
   "source": [
    "# Your written answer here \n",
    "The best ROC is the pink model with all features because it is more close to the top left corner and it is more able to predict more true results compared to the false results, its AUC>.95 which means this model is able to seperate between the true postive data and the false positive with precision above 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T19:26:55.523136Z",
     "start_time": "2020-05-14T19:26:53.879158Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "import pickle\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T19:26:57.928933Z",
     "start_time": "2020-05-14T19:26:57.884247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original classifier has an accuracy score of 0.956.\n",
      "The original classifier has an area under the ROC curve of 0.836.\n"
     ]
    }
   ],
   "source": [
    "network_df = pickle.load(open(\"sample_network_data.pkl\", \"rb\"))\n",
    "\n",
    "# partion features and target \n",
    "X = network_df.drop(\"Purchased\", axis=1)\n",
    "y = network_df[\"Purchased\"]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2019)\n",
    "\n",
    "# scale features\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "# build classifier\n",
    "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model.fit(X_train,y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# get the accuracy score\n",
    "print(f\"The original classifier has an accuracy score of {round(accuracy_score(y_test, y_test_pred), 3)}.\")\n",
    "\n",
    "# get the area under the curve from an ROC curve\n",
    "y_score = model.decision_function(X_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "auc = round(roc_auc_score(y_test, y_score), 3)\n",
    "print(f\"The original classifier has an area under the ROC curve of {auc}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The model above has an accuracy score that might be too good to believe. Using y.value_counts(), explain how y is affecting the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T19:31:28.320745Z",
     "start_time": "2020-05-14T19:31:28.310397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    257\n",
       "1     13\n",
       "Name: Purchased, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your written answer here \n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What methods would you use to address the issues mentioned up above in question 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T20:28:07.531026Z",
     "start_time": "2020-05-13T20:28:07.528443Z"
    }
   },
   "source": [
    "### your written answer here \n",
    "The y is imbalanced because I have more zeros compared to ones. Therefore, the model is able to predict the zero's better than the one's. But this prediction is not very accurate and it weakens its ability to predict the one's.\n",
    "To fix this issue, I can try one or more of the following :\n",
    "\n",
    "1- stratified splitting when split to train and test\n",
    "\n",
    "2- class_weight='balanced' when defining my algorithm \n",
    "\n",
    "3- Use SMOTE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
